{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48691ae-1a2f-4953-8f8c-b37881339825",
   "metadata": {},
   "source": [
    "# Creating a Virtual Webcam Chatbot That Responds to Voice Commands in a Video Meeting\n",
    "In this notebook we test all the functions of the program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa287b5-d6c1-4523-baae-b35dbda936d3",
   "metadata": {},
   "source": [
    "## Converting Audio to Text\n",
    "Using IBM Watson Speech to Text, we'll convert the captured audio into text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf5c40a-13db-4048-8b1c-6d654f352f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import io\n",
    "import os\n",
    "from ibm_watson import SpeechToTextV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "IBM_SPEECH_TO_TEXT_API = os.getenv('IBM_SPEECH_TO_TEXT_API')\n",
    "URL = 'https://api.us-south.speech-to-text.watson.cloud.ibm.com/instances/8006ee0b-b4ed-4e2c-b73e-78f0b03175b4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a2ca531-f3b3-4ad1-911b-dd03d58fe00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method recognize in module ibm_watson.speech_to_text_v1:\n",
      "\n",
      "recognize(audio: <class 'BinaryIO'>, *, content_type: str = None, model: str = None, language_customization_id: str = None, acoustic_customization_id: str = None, base_model_version: str = None, customization_weight: float = None, inactivity_timeout: int = None, keywords: List[str] = None, keywords_threshold: float = None, max_alternatives: int = None, word_alternatives_threshold: float = None, word_confidence: bool = None, timestamps: bool = None, profanity_filter: bool = None, smart_formatting: bool = None, speaker_labels: bool = None, grammar_name: str = None, redaction: bool = None, audio_metrics: bool = None, end_of_phrase_silence_time: float = None, split_transcript_at_phrase_end: bool = None, speech_detector_sensitivity: float = None, background_audio_suppression: float = None, low_latency: bool = None, character_insertion_bias: float = None, **kwargs) -> ibm_cloud_sdk_core.detailed_response.DetailedResponse method of ibm_watson.speech_to_text_v1_adapter.SpeechToTextV1Adapter instance\n",
      "    Recognize audio.\n",
      "    \n",
      "    Sends audio and returns transcription results for a recognition request. You can\n",
      "    pass a maximum of 100 MB and a minimum of 100 bytes of audio with a request. The\n",
      "    service automatically detects the endianness of the incoming audio and, for audio\n",
      "    that includes multiple channels, downmixes the audio to one-channel mono during\n",
      "    transcoding. The method returns only final results; to enable interim results, use\n",
      "    the WebSocket API. (With the `curl` command, use the `--data-binary` option to\n",
      "    upload the file for the request.)\n",
      "    **See also:** [Making a basic HTTP\n",
      "    request](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-http#HTTP-basic).\n",
      "    ### Streaming mode\n",
      "     For requests to transcribe live audio as it becomes available, you must set the\n",
      "    `Transfer-Encoding` header to `chunked` to use streaming mode. In streaming mode,\n",
      "    the service closes the connection (status code 408) if it does not receive at\n",
      "    least 15 seconds of audio (including silence) in any 30-second period. The service\n",
      "    also closes the connection (status code 400) if it detects no speech for\n",
      "    `inactivity_timeout` seconds of streaming audio; use the `inactivity_timeout`\n",
      "    parameter to change the default of 30 seconds.\n",
      "    **See also:**\n",
      "    * [Audio\n",
      "    transmission](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-input#transmission)\n",
      "    *\n",
      "    [Timeouts](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-input#timeouts)\n",
      "    ### Audio formats (content types)\n",
      "     The service accepts audio in the following formats (MIME types).\n",
      "    * For formats that are labeled **Required**, you must use the `Content-Type`\n",
      "    header with the request to specify the format of the audio.\n",
      "    * For all other formats, you can omit the `Content-Type` header or specify\n",
      "    `application/octet-stream` with the header to have the service automatically\n",
      "    detect the format of the audio. (With the `curl` command, you can specify either\n",
      "    `\"Content-Type:\"` or `\"Content-Type: application/octet-stream\"`.)\n",
      "    Where indicated, the format that you specify must include the sampling rate and\n",
      "    can optionally include the number of channels and the endianness of the audio.\n",
      "    * `audio/alaw` (**Required.** Specify the sampling rate (`rate`) of the audio.)\n",
      "    * `audio/basic` (**Required.** Use only with narrowband models.)\n",
      "    * `audio/flac`\n",
      "    * `audio/g729` (Use only with narrowband models.)\n",
      "    * `audio/l16` (**Required.** Specify the sampling rate (`rate`) and optionally the\n",
      "    number of channels (`channels`) and endianness (`endianness`) of the audio.)\n",
      "    * `audio/mp3`\n",
      "    * `audio/mpeg`\n",
      "    * `audio/mulaw` (**Required.** Specify the sampling rate (`rate`) of the audio.)\n",
      "    * `audio/ogg` (The service automatically detects the codec of the input audio.)\n",
      "    * `audio/ogg;codecs=opus`\n",
      "    * `audio/ogg;codecs=vorbis`\n",
      "    * `audio/wav` (Provide audio with a maximum of nine channels.)\n",
      "    * `audio/webm` (The service automatically detects the codec of the input audio.)\n",
      "    * `audio/webm;codecs=opus`\n",
      "    * `audio/webm;codecs=vorbis`\n",
      "    The sampling rate of the audio must match the sampling rate of the model for the\n",
      "    recognition request: for broadband models, at least 16 kHz; for narrowband models,\n",
      "    at least 8 kHz. If the sampling rate of the audio is higher than the minimum\n",
      "    required rate, the service down-samples the audio to the appropriate rate. If the\n",
      "    sampling rate of the audio is lower than the minimum required rate, the request\n",
      "    fails.\n",
      "     **See also:** [Supported audio\n",
      "    formats](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-audio-formats).\n",
      "    ### Next-generation models\n",
      "     The service supports next-generation `Multimedia` (16 kHz) and `Telephony` (8\n",
      "    kHz) models for many languages. Next-generation models have higher throughput than\n",
      "    the service's previous generation of `Broadband` and `Narrowband` models. When you\n",
      "    use next-generation models, the service can return transcriptions more quickly and\n",
      "    also provide noticeably better transcription accuracy.\n",
      "    You specify a next-generation model by using the `model` query parameter, as you\n",
      "    do a previous-generation model. Most next-generation models support the\n",
      "    `low_latency` parameter, and all next-generation models support the\n",
      "    `character_insertion_bias` parameter. These parameters are not available with\n",
      "    previous-generation models.\n",
      "    Next-generation models do not support all of the speech recognition parameters\n",
      "    that are available for use with previous-generation models. Next-generation models\n",
      "    do not support the following parameters:\n",
      "    * `acoustic_customization_id`\n",
      "    * `keywords` and `keywords_threshold`\n",
      "    * `processing_metrics` and `processing_metrics_interval`\n",
      "    * `word_alternatives_threshold`\n",
      "    **Important:** Effective **31 July 2023**, all previous-generation models will be\n",
      "    removed from the service and the documentation. Most previous-generation models\n",
      "    were deprecated on 15 March 2022. You must migrate to the equivalent\n",
      "    next-generation model by 31 July 2023. For more information, see [Migrating to\n",
      "    next-generation\n",
      "    models](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-models-migrate).\n",
      "    **See also:**\n",
      "    * [Next-generation languages and\n",
      "    models](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-models-ng)\n",
      "    * [Supported features for next-generation\n",
      "    models](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-models-ng#models-ng-features)\n",
      "    ### Multipart speech recognition\n",
      "     **Note:** The asynchronous HTTP interface, WebSocket interface, and Watson SDKs\n",
      "    do not support multipart speech recognition.\n",
      "    The HTTP `POST` method of the service also supports multipart speech recognition.\n",
      "    With multipart requests, you pass all audio data as multipart form data. You\n",
      "    specify some parameters as request headers and query parameters, but you pass JSON\n",
      "    metadata as form data to control most aspects of the transcription. You can use\n",
      "    multipart recognition to pass multiple audio files with a single request.\n",
      "    Use the multipart approach with browsers for which JavaScript is disabled or when\n",
      "    the parameters used with the request are greater than the 8 KB limit imposed by\n",
      "    most HTTP servers and proxies. You can encounter this limit, for example, if you\n",
      "    want to spot a very large number of keywords.\n",
      "    **See also:** [Making a multipart HTTP\n",
      "    request](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-http#HTTP-multi).\n",
      "    \n",
      "    :param BinaryIO audio: The audio to transcribe.\n",
      "    :param str content_type: (optional) The format (MIME type) of the audio.\n",
      "           For more information about specifying an audio format, see **Audio formats\n",
      "           (content types)** in the method description.\n",
      "    :param str model: (optional) The model to use for speech recognition. If\n",
      "           you omit the `model` parameter, the service uses the US English\n",
      "           `en-US_BroadbandModel` by default.\n",
      "           _For IBM Cloud Pak for Data,_ if you do not install the\n",
      "           `en-US_BroadbandModel`, you must either specify a model with the request or\n",
      "           specify a new default model for your installation of the service.\n",
      "           **See also:**\n",
      "           * [Using a model for speech\n",
      "           recognition](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-models-use)\n",
      "           * [Using the default\n",
      "           model](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-models-use#models-use-default).\n",
      "    :param str language_customization_id: (optional) The customization ID\n",
      "           (GUID) of a custom language model that is to be used with the recognition\n",
      "           request. The base model of the specified custom language model must match\n",
      "           the model specified with the `model` parameter. You must make the request\n",
      "           with credentials for the instance of the service that owns the custom\n",
      "           model. By default, no custom language model is used. See [Using a custom\n",
      "           language model for speech\n",
      "           recognition](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-languageUse).\n",
      "           **Note:** Use this parameter instead of the deprecated `customization_id`\n",
      "           parameter.\n",
      "    :param str acoustic_customization_id: (optional) The customization ID\n",
      "           (GUID) of a custom acoustic model that is to be used with the recognition\n",
      "           request. The base model of the specified custom acoustic model must match\n",
      "           the model specified with the `model` parameter. You must make the request\n",
      "           with credentials for the instance of the service that owns the custom\n",
      "           model. By default, no custom acoustic model is used. See [Using a custom\n",
      "           acoustic model for speech\n",
      "           recognition](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-acousticUse).\n",
      "    :param str base_model_version: (optional) The version of the specified base\n",
      "           model that is to be used with the recognition request. Multiple versions of\n",
      "           a base model can exist when a model is updated for internal improvements.\n",
      "           The parameter is intended primarily for use with custom models that have\n",
      "           been upgraded for a new base model. The default value depends on whether\n",
      "           the parameter is used with or without a custom model. See [Making speech\n",
      "           recognition requests with upgraded custom\n",
      "           models](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-custom-upgrade-use#custom-upgrade-use-recognition).\n",
      "    :param float customization_weight: (optional) If you specify the\n",
      "           customization ID (GUID) of a custom language model with the recognition\n",
      "           request, the customization weight tells the service how much weight to give\n",
      "           to words from the custom language model compared to those from the base\n",
      "           model for the current request.\n",
      "           Specify a value between 0.0 and 1.0. Unless a different customization\n",
      "           weight was specified for the custom model when the model was trained, the\n",
      "           default value is:\n",
      "           * 0.3 for previous-generation models\n",
      "           * 0.2 for most next-generation models\n",
      "           * 0.1 for next-generation English and Japanese models\n",
      "           A customization weight that you specify overrides a weight that was\n",
      "           specified when the custom model was trained. The default value yields the\n",
      "           best performance in general. Assign a higher value if your audio makes\n",
      "           frequent use of OOV words from the custom model. Use caution when setting\n",
      "           the weight: a higher value can improve the accuracy of phrases from the\n",
      "           custom model's domain, but it can negatively affect performance on\n",
      "           non-domain phrases.\n",
      "           See [Using customization\n",
      "           weight](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-languageUse#weight).\n",
      "    :param int inactivity_timeout: (optional) The time in seconds after which,\n",
      "           if only silence (no speech) is detected in streaming audio, the connection\n",
      "           is closed with a 400 error. The parameter is useful for stopping audio\n",
      "           submission from a live microphone when a user simply walks away. Use `-1`\n",
      "           for infinity. See [Inactivity\n",
      "           timeout](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-input#timeouts-inactivity).\n",
      "    :param List[str] keywords: (optional) An array of keyword strings to spot\n",
      "           in the audio. Each keyword string can include one or more string tokens.\n",
      "           Keywords are spotted only in the final results, not in interim hypotheses.\n",
      "           If you specify any keywords, you must also specify a keywords threshold.\n",
      "           Omit the parameter or specify an empty array if you do not need to spot\n",
      "           keywords.\n",
      "           You can spot a maximum of 1000 keywords with a single request. A single\n",
      "           keyword can have a maximum length of 1024 characters, though the maximum\n",
      "           effective length for double-byte languages might be shorter. Keywords are\n",
      "           case-insensitive.\n",
      "           See [Keyword\n",
      "           spotting](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-spotting#keyword-spotting).\n",
      "    :param float keywords_threshold: (optional) A confidence value that is the\n",
      "           lower bound for spotting a keyword. A word is considered to match a keyword\n",
      "           if its confidence is greater than or equal to the threshold. Specify a\n",
      "           probability between 0.0 and 1.0. If you specify a threshold, you must also\n",
      "           specify one or more keywords. The service performs no keyword spotting if\n",
      "           you omit either parameter. See [Keyword\n",
      "           spotting](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-spotting#keyword-spotting).\n",
      "    :param int max_alternatives: (optional) The maximum number of alternative\n",
      "           transcripts that the service is to return. By default, the service returns\n",
      "           a single transcript. If you specify a value of `0`, the service uses the\n",
      "           default value, `1`. See [Maximum\n",
      "           alternatives](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-metadata#max-alternatives).\n",
      "    :param float word_alternatives_threshold: (optional) A confidence value\n",
      "           that is the lower bound for identifying a hypothesis as a possible word\n",
      "           alternative (also known as \"Confusion Networks\"). An alternative word is\n",
      "           considered if its confidence is greater than or equal to the threshold.\n",
      "           Specify a probability between 0.0 and 1.0. By default, the service computes\n",
      "           no alternative words. See [Word\n",
      "           alternatives](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-spotting#word-alternatives).\n",
      "    :param bool word_confidence: (optional) If `true`, the service returns a\n",
      "           confidence measure in the range of 0.0 to 1.0 for each word. By default,\n",
      "           the service returns no word confidence scores. See [Word\n",
      "           confidence](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-metadata#word-confidence).\n",
      "    :param bool timestamps: (optional) If `true`, the service returns time\n",
      "           alignment for each word. By default, no timestamps are returned. See [Word\n",
      "           timestamps](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-metadata#word-timestamps).\n",
      "    :param bool profanity_filter: (optional) If `true`, the service filters\n",
      "           profanity from all output except for keyword results by replacing\n",
      "           inappropriate words with a series of asterisks. Set the parameter to\n",
      "           `false` to return results with no censoring.\n",
      "           **Note:** The parameter can be used with US English and Japanese\n",
      "           transcription only. See [Profanity\n",
      "           filtering](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-formatting#profanity-filtering).\n",
      "    :param bool smart_formatting: (optional) If `true`, the service converts\n",
      "           dates, times, series of digits and numbers, phone numbers, currency values,\n",
      "           and internet addresses into more readable, conventional representations in\n",
      "           the final transcript of a recognition request. For US English, the service\n",
      "           also converts certain keyword strings to punctuation symbols. By default,\n",
      "           the service performs no smart formatting.\n",
      "           **Note:** The parameter can be used with US English, Japanese, and Spanish\n",
      "           (all dialects) transcription only.\n",
      "           See [Smart\n",
      "           formatting](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-formatting#smart-formatting).\n",
      "    :param bool speaker_labels: (optional) If `true`, the response includes\n",
      "           labels that identify which words were spoken by which participants in a\n",
      "           multi-person exchange. By default, the service returns no speaker labels.\n",
      "           Setting `speaker_labels` to `true` forces the `timestamps` parameter to be\n",
      "           `true`, regardless of whether you specify `false` for the parameter.\n",
      "           * _For previous-generation models,_ the parameter can be used with\n",
      "           Australian English, US English, German, Japanese, Korean, and Spanish (both\n",
      "           broadband and narrowband models) and UK English (narrowband model)\n",
      "           transcription only.\n",
      "           * _For next-generation models,_ the parameter can be used with Czech,\n",
      "           English (Australian, Indian, UK, and US), German, Japanese, Korean, and\n",
      "           Spanish transcription only.\n",
      "           See [Speaker\n",
      "           labels](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-speaker-labels).\n",
      "    :param str grammar_name: (optional) The name of a grammar that is to be\n",
      "           used with the recognition request. If you specify a grammar, you must also\n",
      "           use the `language_customization_id` parameter to specify the name of the\n",
      "           custom language model for which the grammar is defined. The service\n",
      "           recognizes only strings that are recognized by the specified grammar; it\n",
      "           does not recognize other custom words from the model's words resource.\n",
      "           See [Using a grammar for speech\n",
      "           recognition](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-grammarUse).\n",
      "    :param bool redaction: (optional) If `true`, the service redacts, or masks,\n",
      "           numeric data from final transcripts. The feature redacts any number that\n",
      "           has three or more consecutive digits by replacing each digit with an `X`\n",
      "           character. It is intended to redact sensitive numeric data, such as credit\n",
      "           card numbers. By default, the service performs no redaction.\n",
      "           When you enable redaction, the service automatically enables smart\n",
      "           formatting, regardless of whether you explicitly disable that feature. To\n",
      "           ensure maximum security, the service also disables keyword spotting\n",
      "           (ignores the `keywords` and `keywords_threshold` parameters) and returns\n",
      "           only a single final transcript (forces the `max_alternatives` parameter to\n",
      "           be `1`).\n",
      "           **Note:** The parameter can be used with US English, Japanese, and Korean\n",
      "           transcription only.\n",
      "           See [Numeric\n",
      "           redaction](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-formatting#numeric-redaction).\n",
      "    :param bool audio_metrics: (optional) If `true`, requests detailed\n",
      "           information about the signal characteristics of the input audio. The\n",
      "           service returns audio metrics with the final transcription results. By\n",
      "           default, the service returns no audio metrics.\n",
      "           See [Audio\n",
      "           metrics](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-metrics#audio-metrics).\n",
      "    :param float end_of_phrase_silence_time: (optional) Specifies the duration\n",
      "           of the pause interval at which the service splits a transcript into\n",
      "           multiple final results. If the service detects pauses or extended silence\n",
      "           before it reaches the end of the audio stream, its response can include\n",
      "           multiple final results. Silence indicates a point at which the speaker\n",
      "           pauses between spoken words or phrases.\n",
      "           Specify a value for the pause interval in the range of 0.0 to 120.0.\n",
      "           * A value greater than 0 specifies the interval that the service is to use\n",
      "           for speech recognition.\n",
      "           * A value of 0 indicates that the service is to use the default interval.\n",
      "           It is equivalent to omitting the parameter.\n",
      "           The default pause interval for most languages is 0.8 seconds; the default\n",
      "           for Chinese is 0.6 seconds.\n",
      "           See [End of phrase silence\n",
      "           time](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-parsing#silence-time).\n",
      "    :param bool split_transcript_at_phrase_end: (optional) If `true`, directs\n",
      "           the service to split the transcript into multiple final results based on\n",
      "           semantic features of the input, for example, at the conclusion of\n",
      "           meaningful phrases such as sentences. The service bases its understanding\n",
      "           of semantic features on the base language model that you use with a\n",
      "           request. Custom language models and grammars can also influence how and\n",
      "           where the service splits a transcript.\n",
      "           By default, the service splits transcripts based solely on the pause\n",
      "           interval. If the parameters are used together on the same request,\n",
      "           `end_of_phrase_silence_time` has precedence over\n",
      "           `split_transcript_at_phrase_end`.\n",
      "           See [Split transcript at phrase\n",
      "           end](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-parsing#split-transcript).\n",
      "    :param float speech_detector_sensitivity: (optional) The sensitivity of\n",
      "           speech activity detection that the service is to perform. Use the parameter\n",
      "           to suppress word insertions from music, coughing, and other non-speech\n",
      "           events. The service biases the audio it passes for speech recognition by\n",
      "           evaluating the input audio against prior models of speech and non-speech\n",
      "           activity.\n",
      "           Specify a value between 0.0 and 1.0:\n",
      "           * 0.0 suppresses all audio (no speech is transcribed).\n",
      "           * 0.5 (the default) provides a reasonable compromise for the level of\n",
      "           sensitivity.\n",
      "           * 1.0 suppresses no audio (speech detection sensitivity is disabled).\n",
      "           The values increase on a monotonic curve. Specifying one or two decimal\n",
      "           places of precision (for example, `0.55`) is typically more than\n",
      "           sufficient.\n",
      "           The parameter is supported with all next-generation models and with most\n",
      "           previous-generation models. See [Speech detector\n",
      "           sensitivity](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-detection#detection-parameters-sensitivity)\n",
      "           and [Language model\n",
      "           support](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-detection#detection-support).\n",
      "    :param float background_audio_suppression: (optional) The level to which\n",
      "           the service is to suppress background audio based on its volume to prevent\n",
      "           it from being transcribed as speech. Use the parameter to suppress side\n",
      "           conversations or background noise.\n",
      "           Specify a value in the range of 0.0 to 1.0:\n",
      "           * 0.0 (the default) provides no suppression (background audio suppression\n",
      "           is disabled).\n",
      "           * 0.5 provides a reasonable level of audio suppression for general usage.\n",
      "           * 1.0 suppresses all audio (no audio is transcribed).\n",
      "           The values increase on a monotonic curve. Specifying one or two decimal\n",
      "           places of precision (for example, `0.55`) is typically more than\n",
      "           sufficient.\n",
      "           The parameter is supported with all next-generation models and with most\n",
      "           previous-generation models. See [Background audio\n",
      "           suppression](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-detection#detection-parameters-suppression)\n",
      "           and [Language model\n",
      "           support](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-detection#detection-support).\n",
      "    :param bool low_latency: (optional) If `true` for next-generation\n",
      "           `Multimedia` and `Telephony` models that support low latency, directs the\n",
      "           service to produce results even more quickly than it usually does.\n",
      "           Next-generation models produce transcription results faster than\n",
      "           previous-generation models. The `low_latency` parameter causes the models\n",
      "           to produce results even more quickly, though the results might be less\n",
      "           accurate when the parameter is used.\n",
      "           The parameter is not available for previous-generation `Broadband` and\n",
      "           `Narrowband` models. It is available for most next-generation models.\n",
      "           * For a list of next-generation models that support low latency, see\n",
      "           [Supported next-generation language\n",
      "           models](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-models-ng#models-ng-supported).\n",
      "           * For more information about the `low_latency` parameter, see [Low\n",
      "           latency](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-interim#low-latency).\n",
      "    :param float character_insertion_bias: (optional) For next-generation\n",
      "           models, an indication of whether the service is biased to recognize shorter\n",
      "           or longer strings of characters when developing transcription hypotheses.\n",
      "           By default, the service is optimized to produce the best balance of strings\n",
      "           of different lengths.\n",
      "           The default bias is 0.0. The allowable range of values is -1.0 to 1.0.\n",
      "           * Negative values bias the service to favor hypotheses with shorter strings\n",
      "           of characters.\n",
      "           * Positive values bias the service to favor hypotheses with longer strings\n",
      "           of characters.\n",
      "           As the value approaches -1.0 or 1.0, the impact of the parameter becomes\n",
      "           more pronounced. To determine the most effective value for your scenario,\n",
      "           start by setting the value of the parameter to a small increment, such as\n",
      "           -0.1, -0.05, 0.05, or 0.1, and assess how the value impacts the\n",
      "           transcription results. Then experiment with different values as necessary,\n",
      "           adjusting the value by small increments.\n",
      "           The parameter is not available for previous-generation models.\n",
      "           See [Character insertion\n",
      "           bias](https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-parsing#insertion-bias).\n",
      "    :param dict headers: A `dict` containing the request headers\n",
      "    :return: A `DetailedResponse` containing the result, headers and HTTP status code.\n",
      "    :rtype: DetailedResponse with `dict` result representing a `SpeechRecognitionResults` object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "authenticator = IAMAuthenticator(IBM_SPEECH_TO_TEXT_API)\n",
    "speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
    "speech_to_text.set_service_url(URL)\n",
    "help(speech_to_text.recognize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b42c6cc6-a487-44c8-b7e5-a7ea60439c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert audio to text\n",
    "def convert_audio_to_text(audio_data):\n",
    "    authenticator = IAMAuthenticator(IBM_SPEECH_TO_TEXT_API)\n",
    "    speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
    "    speech_to_text.set_service_url(URL)\n",
    "\n",
    "    response = speech_to_text.recognize(\n",
    "        audio=audio_data,\n",
    "        content_type='audio/l16; rate=44100',\n",
    "        model='en-US_BroadbandModel',\n",
    "       # max_alternatives=1,\n",
    "    ).get_result()\n",
    "\n",
    "    return response['results'][0]['alternatives'][0]['transcript']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b5051a-31cf-4c25-9c3f-69d655c90c28",
   "metadata": {},
   "source": [
    "## Create a Function to Transcribe Audio\n",
    "We need a function that captures audio from the microphone in flac format and converts it into text using the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14bd95fc-1ccd-40b3-bfc6-1d12b6b1fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to record audio from the microphone\n",
    "def record_audio():\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 44100\n",
    "    CHUNK = 1024\n",
    "    RECORD_SECONDS = 5\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "    print(\"Recording...\")\n",
    "    frames = []\n",
    "    for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    print(\"Finished recording\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "    audio_data = io.BytesIO(b''.join(frames))\n",
    "    return audio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ecdbddf4-22af-488f-92c4-48f1d344242d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Finished recording\n"
     ]
    }
   ],
   "source": [
    "audio_data = record_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d78415b9-7390-4921-b30b-ccf0e02764be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_io.BytesIO"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d24d47b6-6fb1-44fd-a76c-b96d12203242",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = convert_audio_to_text(audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d7bfdb3-08d0-4074-8c56-e6d08bd22d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'can you recently sent '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ab0ac-d41e-46bd-b145-a85e4079bda5",
   "metadata": {},
   "source": [
    "# New Code Begin here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8fe93c-803d-48dd-b466-4c2706af2058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203ec30-19a0-47f9-adc2-138ae577c036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8a91d-a54f-4b32-8f13-b151dd82d5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538369f-a01d-4060-83c1-da5ac1166234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415ea23-dda1-4c69-9d18-ad30a6330c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import io\n",
    "def play_audio(audio_object):\n",
    "    # Load the audio data from the byte stream object\n",
    "    audio_data, sample_rate = sf.read(io.BytesIO(audio_object.getvalue()))\n",
    "    # Play the audio using sounddevice\n",
    "    sd.play(audio_data, sample_rate)\n",
    "    sd.wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5e499f5e-8664-47ed-a281-9f50d939053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import io\n",
    "def record_audio(duration):\n",
    "    # Set the sample rate and channels\n",
    "    sample_rate = 48000  # You can change this to your desired sample rate\n",
    "    channels = 2  # 1 for mono, 2 for stereo\n",
    "    # Start recording audio from the microphone\n",
    "    audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=channels)\n",
    "    # Wait for the recording to complete\n",
    "    print(\"Recording...\")\n",
    "    sd.wait()\n",
    "    # Create a byte stream object to store the audio data\n",
    "    audio_stream = io.BytesIO()\n",
    "    # Save the recorded audio to the byte stream in the OGG format with Opus codec\n",
    "    sf.write(audio_stream, audio, sample_rate, format='ogg', subtype='opus')\n",
    "    # Move the file pointer to the beginning of the stream\n",
    "    audio_stream.seek(0)\n",
    "    return audio_stream\n",
    "\n",
    "#https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-audio-formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2fb97aeb-9df0-4e0b-a496-4e7a74986708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n"
     ]
    }
   ],
   "source": [
    "# Example usage: record audio for 5 seconds and get the audio as an object\n",
    "duration = 5\n",
    "audio_object = record_audio(duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f610336f-0cfd-41e3-9a87-279eaa2989bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BytesIO at 0x258ffaed6c0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "13bd4bee-55f3-48f4-a46b-375b735de8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: play the audio from the audio object\n",
    "play_audio(audio_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f63eb-9357-4705-8486-6cfb76227130",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "070a8987-2c57-403e-b714-7fac2cdce2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from ibm_watson import SpeechToTextV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "def transcribe_audio(audio_object, api_key, url):\n",
    "    # Create an authenticator object with your IBM Watson Speech to Text API key\n",
    "    authenticator = IAMAuthenticator(api_key)\n",
    "    # Create a Speech to Text object with the authenticator and the service URL\n",
    "    speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
    "    speech_to_text.set_service_url(url)\n",
    "    # Perform the transcription using the audio object\n",
    "    response = speech_to_text.recognize(\n",
    "        audio=audio_object,\n",
    "        content_type='audio/ogg',\n",
    "        model='en-US_Telephony',     \n",
    "    ).get_result()\n",
    "\n",
    "    # Get the transcribed text from the response\n",
    "    transcriptions = response['results']\n",
    "    text = ''\n",
    "    for transcription in transcriptions:\n",
    "        text += transcription['alternatives'][0]['transcript'] + ' '\n",
    "    return text\n",
    "\n",
    " #https://cloud.ibm.com/docs/speech-to-text?topic=speech-to-text-models-ng     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ee496d9-554f-4262-b0a5-a7e86be05c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: transcribe the audio object using IBM Watson Speech to Text\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key= os.getenv('IBM_SPEECH_TO_TEXT_API')\n",
    "url = 'https://api.us-south.speech-to-text.watson.cloud.ibm.com/instances/8006ee0b-b4ed-4e2c-b73e-78f0b03175b4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "58509eaf-8715-40a4-876a-e322be9dc7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello can you listen me  \n"
     ]
    }
   ],
   "source": [
    "#audio_object = record_audio(5)  # Assuming you have a function to record audio and return it as an object\n",
    "transcription = transcribe_audio(audio_object, api_key, url)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9687fab7-2a31-4b9d-b469-2539f9470697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transcription():\n",
    "    # Example usage: transcribe the audio object using IBM Watson Speech to Text\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    api_key= os.getenv('IBM_SPEECH_TO_TEXT_API')\n",
    "    url = 'https://api.us-south.speech-to-text.watson.cloud.ibm.com/instances/8006ee0b-b4ed-4e2c-b73e-78f0b03175b4'\n",
    "    # Example usage: record audio for 5 seconds and get the audio as an object\n",
    "    duration = 5\n",
    "    audio_object = record_audio(duration)    \n",
    "    transcription = transcribe_audio(audio_object, api_key, url)\n",
    "    print(\"Transcript:\", transcription)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fe769d3c-1bed-4456-969f-799f400bad3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Transcript: hello can you listen me  \n"
     ]
    }
   ],
   "source": [
    "test_transcription()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3410eccb-9b7f-4cba-9d2a-6441469e77ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ec9fd7b-b13d-4a6f-9fe2-abdce368f353",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98b077af-09b6-46af-80ef-0fd74f16b121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running:\n",
      "Running:\n",
      "Running:\n",
      "Running:\n",
      "Running:\n",
      "Running:\n",
      "Running:\n",
      "Running:\n",
      "Running:\n",
      "Running:\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import queue\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from dotenv import load_dotenv\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson import SpeechToTextV1\n",
    "\n",
    "# Play audio using sounddevice\n",
    "def play_audio(audio_object):\n",
    "    audio_data, sample_rate = sf.read(io.BytesIO(audio_object.getvalue()))\n",
    "    sd.play(audio_data, sample_rate)\n",
    "    sd.wait()\n",
    "\n",
    "# Record audio and put audio chunks in the queue\n",
    "def record_audio(duration, audio_queue):\n",
    "    sample_rate = 48000\n",
    "    channels = 2\n",
    "    chunk_duration = 0.5\n",
    "    total_chunks = int(duration / chunk_duration)\n",
    "\n",
    "    for _ in range(total_chunks):\n",
    "        audio = sd.rec(int(chunk_duration * sample_rate), samplerate=sample_rate, channels=channels)\n",
    "        sd.wait()\n",
    "        audio_queue.put(audio)\n",
    "\n",
    "# Transcribe audio using IBM Watson Speech to Text\n",
    "def transcribe_audio(audio_object, api_key, url):\n",
    "    authenticator = IAMAuthenticator(api_key)\n",
    "    speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
    "    speech_to_text.set_service_url(url)\n",
    "    response = speech_to_text.recognize(\n",
    "        audio=audio_object,\n",
    "        content_type='audio/ogg',\n",
    "        model='en-US_Telephony',\n",
    "    ).get_result()\n",
    "\n",
    "    transcriptions = response['results']\n",
    "    text = ''\n",
    "    for transcription in transcriptions:\n",
    "        text += transcription['alternatives'][0]['transcript'] + ' '\n",
    "    return text\n",
    "\n",
    "# Main function to record audio in a separate thread and transcribe it in real-time\n",
    "def main():\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv('IBM_SPEECH_TO_TEXT_API')\n",
    "    url = 'https://api.us-south.speech-to-text.watson.cloud.ibm.com/instances/8006ee0b-b4ed-4e2c-b73e-78f0b03175b4'\n",
    "    duration = 5\n",
    "    sample_rate = 48000  # Add this line to define sample_rate\n",
    "    audio_queue = queue.Queue()\n",
    "    audio_thread = threading.Thread(target=record_audio, args=(duration, audio_queue))\n",
    "    audio_thread.start()\n",
    "    while audio_thread.is_alive() or not audio_queue.empty():\n",
    "        if not audio_queue.empty():\n",
    "            audio_chunk = audio_queue.get()\n",
    "            audio_stream = io.BytesIO()\n",
    "            sf.write(audio_stream, audio_chunk, samplerate=sample_rate, format='ogg', subtype='opus')\n",
    "            audio_stream.seek(0)\n",
    "            # Example usage: play the audio from the audio object\n",
    "            play_audio(audio_stream)\n",
    "            print(\"Running:\")\n",
    "            #transcription = transcribe_audio(audio_stream, api_key, url)\n",
    "            #print(\"Transcript:\", transcription)\n",
    "        else:\n",
    "            time.sleep(0.1)\n",
    "    audio_thread.join()\n",
    "if __name__ == \"__main__\":\n",
    "    main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "873e2b99-5b36-45c0-8c9a-8e979ad85d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running:\n",
      "Running:\n",
      "Running:\n",
      "Running:\n",
      "Running:\n",
      "Running:\n",
      "Running:\n",
      "Running:\n",
      "Running:\n",
      "Running:\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c803ed2-8c87-4b9e-bfde-07e7b40344a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "96fd6333-a77b-4e0f-a08d-83478821971d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa872f50-f687-41cc-84b4-183101a81cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e660531-21a8-4c7a-83e1-85051fd40f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No voice detected\n",
      "No voice detected\n",
      "No voice detected\n",
      "No voice detected\n"
     ]
    },
    {
     "ename": "ApiException",
     "evalue": "Error: Stream was 0 bytes but needs to be at least 100 bytes., Code: 400 , X-global-transaction-id: 11be997e-a8bb-42ed-a634-88fbf4b51040",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 84\u001b[0m\n\u001b[0;32m     81\u001b[0m     audio_thread\u001b[38;5;241m.\u001b[39mjoin()\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 74\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m audio_stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m contains_voice(audio_stream):\n\u001b[1;32m---> 74\u001b[0m     transcription \u001b[38;5;241m=\u001b[39m \u001b[43mtranscribe_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranscript:\u001b[39m\u001b[38;5;124m\"\u001b[39m, transcription)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[6], line 34\u001b[0m, in \u001b[0;36mtranscribe_audio\u001b[1;34m(audio_object, api_key, url)\u001b[0m\n\u001b[0;32m     32\u001b[0m speech_to_text \u001b[38;5;241m=\u001b[39m SpeechToTextV1(authenticator\u001b[38;5;241m=\u001b[39mauthenticator)\n\u001b[0;32m     33\u001b[0m speech_to_text\u001b[38;5;241m.\u001b[39mset_service_url(url)\n\u001b[1;32m---> 34\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mspeech_to_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio/ogg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men-US_Telephony\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_result()\n\u001b[0;32m     40\u001b[0m transcriptions \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     41\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\Blog\\Virtual-Webcam-Chatbot\\.venv\\lib\\site-packages\\ibm_watson\\speech_to_text_v1.py:624\u001b[0m, in \u001b[0;36mSpeechToTextV1.recognize\u001b[1;34m(self, audio, content_type, model, language_customization_id, acoustic_customization_id, base_model_version, customization_weight, inactivity_timeout, keywords, keywords_threshold, max_alternatives, word_alternatives_threshold, word_confidence, timestamps, profanity_filter, smart_formatting, speaker_labels, grammar_name, redaction, audio_metrics, end_of_phrase_silence_time, split_transcript_at_phrase_end, speech_detector_sensitivity, background_audio_suppression, low_latency, character_insertion_bias, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/v1/recognize\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    618\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    619\u001b[0m                                url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    620\u001b[0m                                headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    621\u001b[0m                                params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    622\u001b[0m                                data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m--> 624\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Blog\\Virtual-Webcam-Chatbot\\.venv\\lib\\site-packages\\ibm_cloud_sdk_core\\base_service.py:343\u001b[0m, in \u001b[0;36mBaseService.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DetailedResponse(response\u001b[38;5;241m=\u001b[39mresult, headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders, status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;66;03m# Received error status code from server, raise an APIException.\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiException(response\u001b[38;5;241m.\u001b[39mstatus_code, http_response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mSSLError:\n\u001b[0;32m    345\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mERROR_MSG_DISABLE_SSL)\n",
      "\u001b[1;31mApiException\u001b[0m: Error: Stream was 0 bytes but needs to be at least 100 bytes., Code: 400 , X-global-transaction-id: 11be997e-a8bb-42ed-a634-88fbf4b51040"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import queue\n",
    "import threading\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson import SpeechToTextV1\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_nonsilent\n",
    "def play_audio(audio_object):\n",
    "    audio_data, sample_rate = sf.read(io.BytesIO(audio_object.getvalue()))\n",
    "    sd.play(audio_data, sample_rate)\n",
    "    sd.wait()\n",
    "\n",
    "def record_audio(duration, audio_queue):\n",
    "    sample_rate = 48000\n",
    "    channels = 2\n",
    "    chunk_duration = 0.5\n",
    "    total_chunks = int(duration / chunk_duration)\n",
    "\n",
    "    for _ in range(total_chunks):\n",
    "        audio = sd.rec(int(chunk_duration * sample_rate), samplerate=sample_rate, channels=channels)\n",
    "        sd.wait()\n",
    "        audio_queue.put(audio)\n",
    "\n",
    "def transcribe_audio(audio_object, api_key, url):\n",
    "    authenticator = IAMAuthenticator(api_key)\n",
    "    speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
    "    speech_to_text.set_service_url(url)\n",
    "    response = speech_to_text.recognize(\n",
    "        audio=audio_object,\n",
    "        content_type='audio/ogg',\n",
    "        model='en-US_Telephony',\n",
    "    ).get_result()\n",
    "\n",
    "    transcriptions = response['results']\n",
    "    text = ''\n",
    "    for transcription in transcriptions:\n",
    "        text += transcription['alternatives'][0]['transcript'] + ' '\n",
    "    return text\n",
    "\n",
    "def contains_voice(audio_stream, silence_threshold=-50, min_duration=100):\n",
    "    audio_segment = AudioSegment.from_file(audio_stream, format=\"ogg\")\n",
    "    nonsilent_chunks = detect_nonsilent(audio_segment, min_silence_len=min_duration, silence_thresh=silence_threshold)\n",
    "\n",
    "    if len(nonsilent_chunks) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv('IBM_SPEECH_TO_TEXT_API')\n",
    "    url = 'https://api.us-south.speech-to-text.watson.cloud.ibm.com/instances/8006ee0b-b4ed-4e2c-b73e-78f0b03175b4'\n",
    "\n",
    "    duration = 5\n",
    "    sample_rate = 48000\n",
    "    audio_queue = queue.Queue()\n",
    "    audio_thread = threading.Thread(target=record_audio, args=(duration, audio_queue))\n",
    "    audio_thread.start()\n",
    "\n",
    "    while audio_thread.is_alive() or not audio_queue.empty():\n",
    "        if not audio_queue.empty():\n",
    "            audio_chunk = audio_queue.get()\n",
    "            audio_stream = io.BytesIO()\n",
    "            sf.write(audio_stream, audio_chunk, samplerate=sample_rate, format='ogg', subtype='opus')\n",
    "            audio_stream.seek(0)\n",
    "\n",
    "            if contains_voice(audio_stream):\n",
    "                transcription = transcribe_audio(audio_stream, api_key, url)\n",
    "                print(\"Transcript:\", transcription)\n",
    "            else:\n",
    "                print(\"No voice detected\")\n",
    "        else:\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    audio_thread.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc258131-bc99-438c-8c51-e2f413299608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bcbf441-03e2-455a-adac-080ad89e370f",
   "metadata": {},
   "source": [
    "Can you create a new full code with a new main function that is listening the microphone all the time and only if recieves voice play the  complete voice , this is for streaming in real time, when there is a voice the voice is reproduced again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afacbf75-2bec-4935-90aa-dc7727a3b42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No voice detected\n",
      "No voice detected\n",
      "Running:\n",
      "Running:\n",
      "No voice detected\n",
      "No voice detected\n",
      "No voice detected\n",
      "No voice detected\n",
      "Running:\n",
      "Running:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import queue\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from dotenv import load_dotenv\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson import SpeechToTextV1\n",
    "\n",
    "# Play audio using sounddevice\n",
    "def play_audio(audio_object):\n",
    "    audio_data, sample_rate = sf.read(io.BytesIO(audio_object.getvalue()))\n",
    "    sd.play(audio_data, sample_rate)\n",
    "    sd.wait()\n",
    "\n",
    "# Record audio and put audio chunks in the queue\n",
    "def record_audio(duration, audio_queue):\n",
    "    sample_rate = 48000\n",
    "    channels = 2\n",
    "    chunk_duration = 0.5\n",
    "    total_chunks = int(duration / chunk_duration)\n",
    "\n",
    "    for _ in range(total_chunks):\n",
    "        audio = sd.rec(int(chunk_duration * sample_rate), samplerate=sample_rate, channels=channels)\n",
    "        sd.wait()\n",
    "        audio_queue.put(audio)\n",
    "\n",
    "# Transcribe audio using IBM Watson Speech to Text\n",
    "def transcribe_audio(audio_object, api_key, url):\n",
    "    authenticator = IAMAuthenticator(api_key)\n",
    "    speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
    "    speech_to_text.set_service_url(url)\n",
    "    response = speech_to_text.recognize(\n",
    "        audio=audio_object,\n",
    "        content_type='audio/ogg',\n",
    "        model='en-US_Telephony',\n",
    "    ).get_result()\n",
    "\n",
    "    transcriptions = response['results']\n",
    "    text = ''\n",
    "    for transcription in transcriptions:\n",
    "        text += transcription['alternatives'][0]['transcript'] + ' '\n",
    "    return text\n",
    "\n",
    "# Main function to record audio in a separate thread and transcribe it in real-time\n",
    "def main():\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv('IBM_SPEECH_TO_TEXT_API')\n",
    "    url = 'https://api.us-south.speech-to-text.watson.cloud.ibm.com/instances/8006ee0b-b4ed-4e2c-b73e-78f0b03175b4'\n",
    "    duration = 5\n",
    "    sample_rate = 48000  # Add this line to define sample_rate\n",
    "    audio_queue = queue.Queue()\n",
    "    audio_thread = threading.Thread(target=record_audio, args=(duration, audio_queue))\n",
    "    audio_thread.start()\n",
    "\n",
    "    while audio_thread.is_alive() or not audio_queue.empty():\n",
    "        if not audio_queue.empty():\n",
    "            audio_chunk = audio_queue.get()\n",
    "            audio_stream = io.BytesIO()\n",
    "            sf.write(audio_stream, audio_chunk, samplerate=sample_rate, format='ogg', subtype='opus')\n",
    "            audio_stream.seek(0)\n",
    "\n",
    "            if contains_voice(audio_stream):\n",
    "                # Play the audio when voice is detected\n",
    "                play_audio(audio_stream)\n",
    "                print(\"Running:\")\n",
    "                #transcription = transcribe_audio(audio_stream, api_key, url)\n",
    "                #print(\"Transcript:\", transcription)\n",
    "            else:\n",
    "                print(\"No voice detected\")\n",
    "        else:\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    audio_thread.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481e77ab-9056-4173-87af-62aec407af2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d3a9c3d-c54b-4fd8-91cd-48d61a81d43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting voice\n",
      "Running with voice:\n",
      "Waiting voice\n",
      "Running with voice:\n",
      "Waiting voice\n",
      "Running with voice:\n",
      "Waiting voice\n",
      "Running with voice:\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import queue\n",
    "import threading\n",
    "import time\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from dotenv import load_dotenv\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson import SpeechToTextV1\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_nonsilent\n",
    "\n",
    "# Play audio using sounddevice\n",
    "def play_audio(audio_object):\n",
    "    audio_data, sample_rate = sf.read(io.BytesIO(audio_object.getvalue()))\n",
    "    sd.play(audio_data, sample_rate)\n",
    "    sd.wait()\n",
    "\n",
    "# Record audio for the entire duration\n",
    "def record_audio(duration, sample_rate):\n",
    "    audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=2)\n",
    "    sd.wait()\n",
    "    return audio\n",
    "\n",
    "# Transcribe audio using IBM Watson Speech to Text\n",
    "def transcribe_audio(audio_object, api_key, url):\n",
    "    authenticator = IAMAuthenticator(api_key)\n",
    "    speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
    "    speech_to_text.set_service_url(url)\n",
    "    response = speech_to_text.recognize(\n",
    "        audio=audio_object,\n",
    "        content_type='audio/ogg',\n",
    "        model='en-US_Telephony',\n",
    "    ).get_result()\n",
    "\n",
    "    transcriptions = response['results']\n",
    "    text = ''\n",
    "    for transcription in transcriptions:\n",
    "        text += transcription['alternatives'][0]['transcript'] + ' '\n",
    "    return text\n",
    "    \n",
    "\n",
    "# Function to check if a chunk contains_voice\n",
    "def contains_voice(audio_stream, silence_threshold=-50, min_duration=100):\n",
    "    audio_segment = AudioSegment.from_file(audio_stream, format=\"ogg\")\n",
    "    nonsilent_chunks = detect_nonsilent(audio_segment, min_silence_len=min_duration, silence_thresh=silence_threshold)\n",
    "\n",
    "    if len(nonsilent_chunks) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "# Main function to record audio, split it based on silence, and transcribe it in real-time\n",
    "def main():\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv('IBM_SPEECH_TO_TEXT_API')\n",
    "    url = 'https://api.us-south.speech-to-text.watson.cloud.ibm.com/instances/8006ee0b-b4ed-4e2c-b73e-78f0b03175b4'\n",
    "    duration = 10\n",
    "    sample_rate = 48000\n",
    "    silence_duration = 1000  # Minimum duration of silence in milliseconds to split chunks\n",
    "\n",
    "    # Record the entire audio\n",
    "    audio_data = record_audio(duration, sample_rate)\n",
    "    audio_stream = io.BytesIO()\n",
    "    sf.write(audio_stream, audio_data, samplerate=sample_rate, format='ogg', subtype='opus')\n",
    "    audio_stream.seek(0)\n",
    "    audio_segment = AudioSegment.from_file(audio_stream, format=\"ogg\")\n",
    "\n",
    "    # Split audio based on silence\n",
    "    nonsilent_chunks = detect_nonsilent(audio_segment, min_silence_len=silence_duration, silence_thresh=-50)\n",
    "    audio_chunks = [audio_segment[start:end] for (start, end) in nonsilent_chunks]\n",
    "\n",
    "    # Process each audio chunk\n",
    "    for audio_chunk in audio_chunks:\n",
    "        audio_stream = io.BytesIO()\n",
    "        audio_chunk.export(audio_stream, format=\"ogg\")\n",
    "        audio_stream.seek(0)\n",
    "        print(\"Waiting voice\")\n",
    "        if contains_voice(audio_stream):\n",
    "            # Play the audio when voice is detected\n",
    "            print(\"Running with voice:\")\n",
    "            play_audio(audio_stream)\n",
    "            \n",
    "            #transcription = transcribe_audio(audio_stream, api_key, url)\n",
    "            #print(\"Transcript:\", transcription)\n",
    "        else:\n",
    "            print(\"No voice detected\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88ebd23-1563-4e74-a3bc-0b820ca91ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62fab89f-651b-4ea1-b717-aee1ef0b45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import os\n",
    "import queue\n",
    "import threading\n",
    "import time\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from dotenv import load_dotenv\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson import SpeechToTextV1\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_nonsilent\n",
    "\n",
    "# Play audio using sounddevice\n",
    "def play_audio(audio_object):\n",
    "    audio_data, sample_rate = sf.read(io.BytesIO(audio_object.getvalue()))\n",
    "    sd.play(audio_data, sample_rate)\n",
    "    sd.wait()\n",
    "\n",
    "# Record audio for the entire duration\n",
    "def record_audio(duration, sample_rate):\n",
    "    audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=2)\n",
    "    sd.wait()\n",
    "    return audio\n",
    "\n",
    "# Transcribe audio using IBM Watson Speech to Text\n",
    "def transcribe_audio(audio_object, api_key, url):\n",
    "    authenticator = IAMAuthenticator(api_key)\n",
    "    speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
    "    speech_to_text.set_service_url(url)\n",
    "    response = speech_to_text.recognize(\n",
    "        audio=audio_object,\n",
    "        content_type='audio/ogg',\n",
    "        model='en-US_Telephony',\n",
    "    ).get_result()\n",
    "\n",
    "    transcriptions = response['results']\n",
    "    text = ''\n",
    "    for transcription in transcriptions:\n",
    "        text += transcription['alternatives'][0]['transcript'] + ' '\n",
    "    return text\n",
    "    \n",
    "\n",
    "# Function to check if a chunk contains_voice\n",
    "def contains_voice(audio_stream, silence_threshold=-50, min_duration=100):\n",
    "    audio_segment = AudioSegment.from_file(audio_stream, format=\"ogg\")\n",
    "    nonsilent_chunks = detect_nonsilent(audio_segment, min_silence_len=min_duration, silence_thresh=silence_threshold)\n",
    "\n",
    "    if len(nonsilent_chunks) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "# Main function to record audio, split it based on silence, and transcribe it in real-time\n",
    "def main():\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv('IBM_SPEECH_TO_TEXT_API')\n",
    "    url = 'https://api.us-south.speech-to-text.watson.cloud.ibm.com/instances/8006ee0b-b4ed-4e2c-b73e-78f0b03175b4'\n",
    "    duration = 10\n",
    "    sample_rate = 48000\n",
    "    silence_duration = 1000  # Minimum duration of silence in milliseconds to split chunks\n",
    "    # Record the entire audio\n",
    "    audio_data = record_audio(duration, sample_rate)\n",
    "    audio_stream = io.BytesIO()\n",
    "    sf.write(audio_stream, audio_data, samplerate=sample_rate, format='ogg', subtype='opus')\n",
    "    audio_stream.seek(0)\n",
    "    audio_segment = AudioSegment.from_file(audio_stream, format=\"ogg\")\n",
    "    # Play the audio_segment before splitting it\n",
    "    audio_stream.seek(0)\n",
    "    play_audio(audio_stream)\n",
    "if __name__ == \"__main__\":\n",
    "    main()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf1a037f-318d-4486-839c-ea38fa394482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06374587-7bf4-4a7b-bf80-867de2137f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored from cffi callback <function SoundFile._init_virtual_io.<locals>.vio_tell at 0x0000022A21ACB130>:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\066226758\\Blog\\Virtual-Webcam-Chatbot\\.venv\\lib\\site-packages\\soundfile.py\", line 1264, in vio_tell\n",
      "    @_ffi.callback(\"sf_vio_tell\")\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "LibsndfileError",
     "evalue": "Error opening <_io.BytesIO object at 0x0000022A1F434DB0>: Unspecified internal error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m             play_audio(audio_stream)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 46\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[42], line 43\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     audio_stream \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m---> 43\u001b[0m     \u001b[43mplay_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_stream\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[42], line 11\u001b[0m, in \u001b[0;36mplay_audio\u001b[1;34m(audio_object)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplay_audio\u001b[39m(audio_object):\n\u001b[1;32m---> 11\u001b[0m     audio_data, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     sd\u001b[38;5;241m.\u001b[39mplay(audio_data, sample_rate)\n\u001b[0;32m     13\u001b[0m     sd\u001b[38;5;241m.\u001b[39mwait()\n",
      "File \u001b[1;32m~\\Blog\\Virtual-Webcam-Chatbot\\.venv\\lib\\site-packages\\soundfile.py:285\u001b[0m, in \u001b[0;36mread\u001b[1;34m(file, frames, start, stop, dtype, always_2d, fill_value, out, samplerate, channels, format, subtype, endian, closefd)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(file, frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m, always_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    200\u001b[0m          fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, samplerate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    201\u001b[0m          \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, subtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, endian\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    202\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Provide audio data from a sound file as NumPy array.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m    By default, the whole file is read from the beginning, but the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    283\u001b[0m \n\u001b[0;32m    284\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msubtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    287\u001b[0m         frames \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39m_prepare_read(start, stop, frames)\n\u001b[0;32m    288\u001b[0m         data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread(frames, dtype, always_2d, fill_value, out)\n",
      "File \u001b[1;32m~\\Blog\\Virtual-Webcam-Chatbot\\.venv\\lib\\site-packages\\soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[0;32m    661\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\Blog\\Virtual-Webcam-Chatbot\\.venv\\lib\\site-packages\\soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_ptr \u001b[38;5;241m==\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mNULL:\n\u001b[0;32m   1214\u001b[0m     \u001b[38;5;66;03m# get the actual error code\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n\u001b[0;32m   1221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening <_io.BytesIO object at 0x0000022A1F434DB0>: Unspecified internal error."
     ]
    }
   ],
   "source": [
    "import io\n",
    "import queue\n",
    "import threading\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import detect_nonsilent\n",
    "\n",
    "# Play audio using sounddevice\n",
    "def play_audio(audio_object):\n",
    "    audio_data, sample_rate = sf.read(io.BytesIO(audio_object.getvalue()))\n",
    "    sd.play(audio_data, sample_rate)\n",
    "    sd.wait()\n",
    "\n",
    "# Function to check if a chunk contains_voice\n",
    "def contains_voice(audio_stream, silence_threshold=-50, min_duration=100):\n",
    "    audio_segment = AudioSegment.from_file(audio_stream, format=\"ogg\")\n",
    "    nonsilent_chunks = detect_nonsilent(audio_segment, min_silence_len=min_duration, silence_thresh=silence_threshold)\n",
    "\n",
    "    if len(nonsilent_chunks) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Callback function to process audio in real-time\n",
    "def callback(indata, frames, time, status, q):\n",
    "    audio_stream = io.BytesIO()\n",
    "    sf.write(audio_stream, indata, samplerate=48000, format='ogg', subtype='opus')\n",
    "    audio_stream.seek(0)\n",
    "    if contains_voice(audio_stream):\n",
    "        q.put(audio_stream)\n",
    "\n",
    "# Main function to listen to microphone and play audio when voice is detected\n",
    "def main():\n",
    "    sample_rate = 48000\n",
    "    chunk_size = 2048\n",
    "\n",
    "    q = queue.Queue()\n",
    "    with sd.InputStream(samplerate=sample_rate, channels=2, blocksize=chunk_size, callback=lambda i, f, t, s: callback(i, f, t, s, q)):\n",
    "        print(\"Listening...\")\n",
    "        while True:\n",
    "            audio_stream = q.get()\n",
    "            play_audio(audio_stream)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5cc90f-223a-499b-a1c7-fbb7f6af989d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d4ad42-0955-4178-9e9a-e0d58fa2929a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737456c7-79f0-4cf4-9abb-2122b2f35c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8acab57-8782-48a8-84cd-0082a5d2528e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d339c4f2-f5ee-404a-a5df-d791b4bc1e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190044a5-ece3-4afd-bd5b-51f7d22aa131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd65f54-84b8-428f-9dd4-dbb0b0193156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17097039-5bca-4b9f-a591-85781fb0efd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fb54e85d-7123-4a53-974d-d43b48e498ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyaudio\n",
    "import queue\n",
    "import threading\n",
    "import json\n",
    "from ibm_watson import SpeechToTextV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Global variables\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "AUDIO_QUEUE = queue.Queue()\n",
    "STOP_FLAG = threading.Event()\n",
    "\n",
    "def audio_capture(stream):\n",
    "    while not STOP_FLAG.is_set():\n",
    "        audio_data = stream.read(CHUNK)\n",
    "        AUDIO_QUEUE.put(audio_data)\n",
    "\n",
    "def transcribe_audio(audio_object, api_key, url):\n",
    "    # Create an authenticator object with your IBM Watson Speech to Text API key\n",
    "    authenticator = IAMAuthenticator(api_key)\n",
    "\n",
    "    # Create a Speech to Text object with the authenticator and the service URL\n",
    "    speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
    "    speech_to_text.set_service_url(url)\n",
    "\n",
    "    # Perform the transcription using the audio object\n",
    "    response = speech_to_text.recognize(\n",
    "        audio=audio_object,\n",
    "        content_type='audio/ogg',\n",
    "        model='en-US_Telephony',\n",
    "    ).get_result()\n",
    "\n",
    "    # Get the transcribed text from the response\n",
    "    transcriptions = response['results']\n",
    "    text = ''\n",
    "    for transcription in transcriptions:\n",
    "        text += transcription['alternatives'][0]['transcript'] + ' '\n",
    "\n",
    "    return text\n",
    "\n",
    "def main():\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv('IBM_SPEECH_TO_TEXT_API')\n",
    "    url = 'https://api.us-south.speech-to-text.watson.cloud.ibm.com/instances/8006ee0b-b4ed-4e2c-b73e-78f0b03175b4'\n",
    "\n",
    "    # Create a PyAudio object for audio capture\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    # Open an audio stream for capturing\n",
    "    stream = audio.open(\n",
    "        format=FORMAT,\n",
    "        channels=CHANNELS,\n",
    "        rate=RATE,\n",
    "        input=True,\n",
    "        frames_per_buffer=CHUNK\n",
    "    )\n",
    "\n",
    "    # Start a separate thread for audio capture\n",
    "    capture_thread = threading.Thread(target=audio_capture, args=(stream,))\n",
    "    capture_thread.start()\n",
    "\n",
    "    # Continuously transcribe audio from the queue\n",
    "    while True:\n",
    "        if not AUDIO_QUEUE.empty():\n",
    "            audio_data = AUDIO_QUEUE.get()\n",
    "\n",
    "            # Create a byte stream object to store the audio data\n",
    "            audio_stream = io.BytesIO()\n",
    "            sf.write(audio_stream, audio_data, RATE, format='ogg', subtype='opus')\n",
    "\n",
    "            # Transcribe the audio\n",
    "            transcription = transcribe_audio(audio_stream, api_key, url)\n",
    "            print(transcription)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0a15f376-f9bf-4c5d-b2a4-642b0de40678",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "#    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ad741c20-e252-48a6-afea-2d82ade7d91b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[91], line 74\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Create a byte stream object to store the audio data\u001b[39;00m\n\u001b[0;32m     73\u001b[0m audio_stream \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[1;32m---> 74\u001b[0m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mogg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Transcribe the audio\u001b[39;00m\n\u001b[0;32m     77\u001b[0m transcription \u001b[38;5;241m=\u001b[39m transcribe_audio(audio_stream, api_key, url)\n",
      "File \u001b[1;32m~\\Blog\\Virtual-Webcam-Chatbot\\.venv\\lib\\site-packages\\soundfile.py:342\u001b[0m, in \u001b[0;36mwrite\u001b[1;34m(file, data, samplerate, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    340\u001b[0m     channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 342\u001b[0m     channels \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SoundFile(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m, samplerate, channels,\n\u001b[0;32m    344\u001b[0m                subtype, endian, \u001b[38;5;28mformat\u001b[39m, closefd) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    345\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(data)\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "27314c11-7d17-44f5-823f-905d470de452",
   "metadata": {},
   "outputs": [
    {
     "ename": "ApiException",
     "evalue": "Error: Unable to transcode from audio/l16; rate=48000; endianness=little to one of: audio/l16; rate=8000; channels=1, application/srgs, application/srgs+xml, Code: 415 , X-global-transaction-id: aca95c48-b946-4d7d-8840-c8d27470d14b",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 83\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[38;5;28mprint\u001b[39m(transcription)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 83\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[119], line 78\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m audio_stream \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(audio_data)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Transcribe the audio\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m transcription \u001b[38;5;241m=\u001b[39m \u001b[43mtranscribe_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(transcription)\n",
      "Cell \u001b[1;32mIn[119], line 34\u001b[0m, in \u001b[0;36mtranscribe_audio\u001b[1;34m(audio_object, api_key, url)\u001b[0m\n\u001b[0;32m     31\u001b[0m speech_to_text\u001b[38;5;241m.\u001b[39mset_service_url(url)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Perform the transcription using the audio object\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mspeech_to_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio/l16; rate=48000; endianness=little\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men-US_Telephony\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_result()\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Get the transcribed text from the response\u001b[39;00m\n\u001b[0;32m     41\u001b[0m transcriptions \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\Blog\\Virtual-Webcam-Chatbot\\.venv\\lib\\site-packages\\ibm_watson\\speech_to_text_v1.py:624\u001b[0m, in \u001b[0;36mSpeechToTextV1.recognize\u001b[1;34m(self, audio, content_type, model, language_customization_id, acoustic_customization_id, base_model_version, customization_weight, inactivity_timeout, keywords, keywords_threshold, max_alternatives, word_alternatives_threshold, word_confidence, timestamps, profanity_filter, smart_formatting, speaker_labels, grammar_name, redaction, audio_metrics, end_of_phrase_silence_time, split_transcript_at_phrase_end, speech_detector_sensitivity, background_audio_suppression, low_latency, character_insertion_bias, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/v1/recognize\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    618\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    619\u001b[0m                                url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    620\u001b[0m                                headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    621\u001b[0m                                params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    622\u001b[0m                                data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m--> 624\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\Blog\\Virtual-Webcam-Chatbot\\.venv\\lib\\site-packages\\ibm_cloud_sdk_core\\base_service.py:343\u001b[0m, in \u001b[0;36mBaseService.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DetailedResponse(response\u001b[38;5;241m=\u001b[39mresult, headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders, status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;66;03m# Received error status code from server, raise an APIException.\u001b[39;00m\n\u001b[1;32m--> 343\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ApiException(response\u001b[38;5;241m.\u001b[39mstatus_code, http_response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mSSLError:\n\u001b[0;32m    345\u001b[0m     logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mERROR_MSG_DISABLE_SSL)\n",
      "\u001b[1;31mApiException\u001b[0m: Error: Unable to transcode from audio/l16; rate=48000; endianness=little to one of: audio/l16; rate=8000; channels=1, application/srgs, application/srgs+xml, Code: 415 , X-global-transaction-id: aca95c48-b946-4d7d-8840-c8d27470d14b"
     ]
    }
   ],
   "source": [
    "\n",
    "import pyaudio\n",
    "import queue\n",
    "import threading\n",
    "import json\n",
    "from ibm_watson import SpeechToTextV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import soundfile as sf\n",
    "import io\n",
    "\n",
    "# Global variables\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE =  8000 \n",
    "AUDIO_QUEUE = queue.Queue()\n",
    "STOP_FLAG = threading.Event()\n",
    "\n",
    "def audio_capture(stream):\n",
    "    while not STOP_FLAG.is_set():\n",
    "        audio_data = stream.read(CHUNK)\n",
    "        AUDIO_QUEUE.put(audio_data)\n",
    "\n",
    "def transcribe_audio(audio_object, api_key, url):\n",
    "    # Create an authenticator object with your IBM Watson Speech to Text API key\n",
    "    authenticator = IAMAuthenticator(api_key)\n",
    "\n",
    "    # Create a Speech to Text object with the authenticator and the service URL\n",
    "    speech_to_text = SpeechToTextV1(authenticator=authenticator)\n",
    "    speech_to_text.set_service_url(url)\n",
    "\n",
    "    # Perform the transcription using the audio object\n",
    "    response = speech_to_text.recognize(\n",
    "        audio=audio_object.getvalue(),\n",
    "        content_type='audio/l16; rate=48000; endianness=little',\n",
    "        model='en-US_Telephony',\n",
    "    ).get_result()\n",
    "\n",
    "    # Get the transcribed text from the response\n",
    "    transcriptions = response['results']\n",
    "    text = ''\n",
    "    for transcription in transcriptions:\n",
    "        text += transcription['alternatives'][0]['transcript'] + ' '\n",
    "\n",
    "    return text\n",
    "\n",
    "def main():\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv('IBM_SPEECH_TO_TEXT_API')\n",
    "    url = 'https://api.us-south.speech-to-text.watson.cloud.ibm.com/instances/8006ee0b-b4ed-4e2c-b73e-78f0b03175b4'\n",
    "\n",
    "    # Create a PyAudio object for audio capture\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    # Open an audio stream for capturing\n",
    "    stream = audio.open(\n",
    "        format=FORMAT,\n",
    "        channels=CHANNELS,\n",
    "        rate=RATE,\n",
    "        input=True,\n",
    "        frames_per_buffer=CHUNK\n",
    "    )\n",
    "\n",
    "    # Start a separate thread for audio capture\n",
    "    capture_thread = threading.Thread(target=audio_capture, args=(stream,))\n",
    "    capture_thread.start()\n",
    "\n",
    "    # Continuously transcribe audio from the queue\n",
    "    while True:\n",
    "        if not AUDIO_QUEUE.empty():\n",
    "            audio_data = AUDIO_QUEUE.get()\n",
    "\n",
    "            # Create a byte stream object to store the audio data\n",
    "            audio_stream = io.BytesIO(audio_data)\n",
    "\n",
    "            # Transcribe the audio\n",
    "            transcription = transcribe_audio(audio_stream, api_key, url)\n",
    "            print(transcription)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a79aca-452c-4733-966f-658ef6701f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66bfcdf-fc1d-41d4-bb77-54233fd10c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cf1be5-2031-4c0f-b2f4-29bc2916b619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d936181b-f935-4c8e-af71-6c851e42caf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbab4a4-affd-450f-b26a-f82cedec7eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0a151-513d-4527-a27b-d658299a1c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd34c3-5819-45fb-adb2-15caf9502db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef5cbf-2633-4fe7-8cad-7b40c8c11459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03446f0a-5530-4c28-b528-a6648ca02ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e53f6-7775-43ef-956c-1f2dcd80da34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524c5c92-2b5d-434d-ba82-add33ff68794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3845b9f-40b7-4ca9-8c7a-2e6077ede5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "def listen_and_transcribe(source_type=\"microphone\"):\n",
    "    recognizer = sr.Recognizer()\n",
    "    if source_type == \"microphone\":\n",
    "        with sr.Microphone() as source:\n",
    "            print(\"Listening...\")\n",
    "            audio = recognizer.listen(source)\n",
    "    elif source_type == \"virtual_audio_cable\":\n",
    "        with sr.Microphone(device_index=1) as source:  # Adjust device_index based on your system configuration\n",
    "            print(\"Listening...\")\n",
    "            audio = recognizer.listen(source)\n",
    "\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        print(f\"Transcribed: {text}\")\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Google Speech Recognition could not understand the audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "    return None\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece1df5-0e87-47ab-b534-94142409e472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a138079-8bc8-4980-9dc2-cd929de7cdde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa4c84a0-bf8e-4730-a847-48e009d13259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import threading\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6aebc36-7633-4d1c-a2a6-c57825ae55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio settings\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 44100\n",
    "CHUNK = 1024\n",
    "RECORD_SECONDS = 5\n",
    "\n",
    "# Create audio stream queue\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Create a flag to indicate when to trigger ChatGPT\n",
    "trigger_chatgpt = False\n",
    "\n",
    "# Function to capture audio and add it to the queue\n",
    "def capture_audio():\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"Capturing audio...\")\n",
    "\n",
    "    while True:\n",
    "        frames = []\n",
    "        for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "            data = stream.read(CHUNK)\n",
    "            frames.append(data)\n",
    "        audio_queue.put(b''.join(frames))\n",
    "\n",
    "        # Check if the word \"computer\" is said\n",
    "        if b'computer' in b''.join(frames):\n",
    "            trigger_chatgpt = True\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c56774e-04d9-4fa1-8327-f049117edc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing audio...\n"
     ]
    }
   ],
   "source": [
    "# Start capturing audio in a separate thread\n",
    "audio_thread = threading.Thread(target=capture_audio)\n",
    "audio_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc4348-7c72-4ead-985e-b05739ad7c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (watson)",
   "language": "python",
   "name": "watson"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
